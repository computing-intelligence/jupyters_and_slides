{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import  matplotlib.pyplot  as plt\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wikipedia,  smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行 python WikiExtractor.py -b 2000M zhwiki-20181101-pages-articles.xml.bz2\n",
    "# 没有做繁体转简体处理，不会安装opencc的windows版本 :-("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f5051efa71b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_wiki_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D://pyproject//git//AI-NLP//data//text//AA//wiki_00'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mall_wiki_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'<[^>]+>'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_wiki_content\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 去掉 <doc> tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_wiki_content = open('D://pyproject//git//AI-NLP//data//text//AA//wiki_00',encoding='UTF-8').read()\n",
    "all_wiki_content = re.sub(r'<[^>]+>','',all_wiki_content) # 去掉 <doc> tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在8g内存的电脑上能跑完，但在内存小的电脑上报MemeoryError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string): \n",
    "    #return ''.join(re.findall('[\\w|\\d]+', string)) # 不太明白两种写法的区别，w也能匹配数字，但输出结果是有不同\n",
    "    return ''.join(re.findall('\\w+', string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380434793"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_character = tokenize(all_wiki_content)\n",
    "del all_wiki_content # 释放内存\n",
    "len(all_character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 9938192),\n",
       " ('1', 5740539),\n",
       " ('0', 4559519),\n",
       " ('年', 4088849),\n",
       " ('2', 3705103),\n",
       " ('一', 3174566),\n",
       " ('在', 3142422),\n",
       " ('是', 2800422),\n",
       " ('中', 2763222),\n",
       " ('9', 2730241),\n",
       " ('人', 2610319),\n",
       " ('大', 2095073),\n",
       " ('有', 2064509),\n",
       " ('e', 1885083),\n",
       " ('a', 1789303),\n",
       " ('3', 1753587),\n",
       " ('5', 1721315),\n",
       " ('和', 1705550),\n",
       " ('為', 1662714),\n",
       " ('8', 1646008)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_character_counts = Counter(all_character)\n",
    "all_character_counts.most_common()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('20', 1579014),\n",
       " ('19', 1442094),\n",
       " ('00', 1225241),\n",
       " ('01', 853922),\n",
       " ('10', 547006),\n",
       " ('年1', 527492),\n",
       " ('的一', 469028),\n",
       " ('12', 444080),\n",
       " ('11', 419457),\n",
       " ('0年', 417267),\n",
       " ('一个', 400248),\n",
       " ('18', 387729),\n",
       " ('人口', 349391),\n",
       " ('99', 340092),\n",
       " ('中国', 328509),\n",
       " ('1年', 322136),\n",
       " ('公里', 320126),\n",
       " ('5年', 318534),\n",
       " ('月1', 318147),\n",
       " ('er', 316517)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gram_length = 2\n",
    "two_gram_counts = {}\n",
    "for i in range(len(all_character)-gram_length): # 用 for循环省内存\n",
    "    k = all_character[i:i+gram_length]\n",
    "    two_gram_counts[k] = (two_gram_counts[k]+ 1) if (k in two_gram_counts.keys()) else 1\n",
    "    \n",
    "two_gram_counts = Counter(two_gram_counts)\n",
    "two_gram_counts.most_common()[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram, Good-Turing smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21491"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_character_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_prob_from_counts(counts,k=5): \n",
    "    # Good-Turing smoothing\n",
    "    n = sum(counts.values())\n",
    "    # 计算nr\n",
    "    nr = defaultdict(int)\n",
    "    for i in counts.values():\n",
    "        nr[i] += 1\n",
    "    nr[0] = 90000 - len(counts) # 假设汉字共9万个\n",
    "    # 计算rstar\n",
    "    rstar = [0]*(k+1) \n",
    "    total_decreased = 0\n",
    "    for r in range(1,k+1,1):\n",
    "        rstar[r] = (r+1)*nr[r+1]/nr[r]\n",
    "        total_decreased += (r*nr[r] - rstar[r]*nr[r])\n",
    "        #print(r,rstar[r])\n",
    "    rstar[0] = total_decreased / nr[0]\n",
    "    #print (0,rstar[0])\n",
    "    def get_prob(char):\n",
    "        occurence = counts.get(char,0)\n",
    "        return rstar[occurence]/n if occurence<=k else occurence/n\n",
    "    return get_prob\n",
    "\n",
    "get_char_prob = get_char_prob_from_counts(all_character_counts,k=5)\n",
    "\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "def get_1_gram_string_prob(string):\n",
    "    return reduce(mul,[get_char_prob(char) for char in string])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = \"\"\"前天晚上吃晚饭的时候\n",
    "前天晚上吃早饭的时候\"\"\".split('\\n')\n",
    "\n",
    "pair2 = \"\"\"正是一个好看的小猫\n",
    "真是一个好看的小猫\"\"\".split('\\n')\n",
    "\n",
    "pair3 = \"\"\"我无言以对，简直\n",
    "我简直无言以对\"\"\".split('\\n')\n",
    "\n",
    "pairs = [pair, pair2, pair3]\n",
    "def get_probability_prefromance(language_model_func, pairs):\n",
    "    for (p1, p2) in pairs:\n",
    "        print('*'*18)\n",
    "        print('\\t\\t {} with probability {}'.format(p1, language_model_func(tokenize(p1)))) # tokenize去掉','这样的标点\n",
    "        print('\\t\\t {} with probability {}'.format(p2, language_model_func(tokenize(p2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************\n",
      "\t\t 前天晚上吃晚饭的时候 with probability 2.3223066267509665e-33\n",
      "\t\t 前天晚上吃早饭的时候 with probability 4.678562566970852e-33\n",
      "******************\n",
      "\t\t 正是一个好看的小猫 with probability 1.1087006396816684e-26\n",
      "\t\t 真是一个好看的小猫 with probability 3.4663369707956e-27\n",
      "******************\n",
      "\t\t 我无言以对，简直 with probability 1.747335364002409e-23\n",
      "\t\t 我简直无言以对 with probability 1.747335364002409e-23\n"
     ]
    }
   ],
   "source": [
    "get_probability_prefromance(get_1_gram_string_prob, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-gram, Katz back-off smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_gram_table ={}\n",
    "for w in two_gram_counts.keys():\n",
    "    if w[0] not in two_gram_table.keys():\n",
    "        two_gram_table[w[0]] = {}\n",
    "    two_gram_table[w[0]][w[1]] = two_gram_counts[w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_gram_prob_from_counts(counts,k=5): \n",
    "    # Katz smoothing\n",
    "    n = sum(counts.values())\n",
    "    # 计算nr\n",
    "    nr = defaultdict(int)\n",
    "    for i in counts.values():\n",
    "        nr[i] += 1\n",
    "\n",
    "    # 计算dr, for 1<= r<=k\n",
    "    dr = [1]*(k+1) # dr[0] 不使用\n",
    "    tmp = (k+1)*nr[k+1]/nr[1]\n",
    "    for r in range(1,k+1,1):\n",
    "        rstar = (r+1)*nr[r+1]/nr[r]\n",
    "        dr[r] = (rstar/r-tmp)/(1-tmp)\n",
    "\n",
    "    # 计算 two_gram_table，最终存储了所有pair的条件概率\n",
    "    # 计算 a,回退到unigram的系数\n",
    "    two_gram_table ={}\n",
    "    a = {}\n",
    "    for w in counts.keys():\n",
    "        if w[0] not in two_gram_table.keys():\n",
    "            two_gram_table[w[0]] = {}\n",
    "        two_gram_table[w[0]][w[1]] = counts[w]\n",
    "    for w0 in two_gram_table.keys():\n",
    "        n0 = sum(two_gram_table[w0].values())\n",
    "        for w1 in two_gram_table[w0].keys():\n",
    "            c = two_gram_table[w0][w1]\n",
    "            if c > k:\n",
    "                two_gram_table[w0][w1] = c/n0\n",
    "            else:\n",
    "                two_gram_table[w0][w1] = dr[c]*c/n0\n",
    "        sumkatz = sum(two_gram_table[w0].values())\n",
    "        sumSeenUnigram = sum(get_char_prob(e) for e in two_gram_table[w0].keys())\n",
    "        a[w0] = (1-sumkatz)/(1-sumSeenUnigram)\n",
    "        \n",
    "     \n",
    "    def get_prob(word,prev):\n",
    "        occurence = counts.get(prev+word,0)\n",
    "        if occurence > 0:\n",
    "            return two_gram_table[prev][word]\n",
    "        elif prev == '<s>':\n",
    "            return get_char_prob(word)\n",
    "        else:\n",
    "            return a[prev]*get_char_prob(word)       \n",
    "    return get_prob\n",
    "\n",
    "get_2_gram_prob = get_2_gram_prob_from_counts(two_gram_counts,k=5)\n",
    "\n",
    "def get_2_gram_string_prob(string):\n",
    "    probList = []\n",
    "    for i,c in enumerate(string):\n",
    "        prev = '<s>' if i == 0 else string[i-1]\n",
    "        probList.append(get_2_gram_prob(c,prev))\n",
    "    return reduce(mul,probList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************\n",
      "\t\t 前天晚上吃晚饭的时候 with probability 2.8863673714341063e-24\n",
      "\t\t 前天晚上吃早饭的时候 with probability 6.446398053347142e-25\n",
      "******************\n",
      "\t\t 正是一个好看的小猫 with probability 6.4603668266917246e-21\n",
      "\t\t 真是一个好看的小猫 with probability 9.252304558785208e-22\n",
      "******************\n",
      "\t\t 我无言以对，简直 with probability 9.167158643679254e-21\n",
      "\t\t 我简直无言以对 with probability 7.569514628385958e-22\n"
     ]
    }
   ],
   "source": [
    "get_probability_prefromance(get_2_gram_string_prob, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-gram 和 smoothing 并没能很好的区分这些语句。感觉问题主要是上下文的距离超过了2-gram的长度。分词后，再建立3-gram模型，或许能够区分开来。"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
